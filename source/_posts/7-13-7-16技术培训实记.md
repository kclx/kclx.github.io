---
title: 7.13-7.16技术培训实记
tags: [ ]
categories: [ Computer Technology ]
top: false
comments: true
lang: en
toc: true
excerpt: 7.13-7.16技术培训实记
swiper: false
swiperDesc: 7.13-7.16技术培训实记
tocOpen: true
onlyTitle: false
share: true
copyright: true
donate: true
bgImgTransition: fade
bgImgDelay: 180000
prismjs: default
mathjax: false
imgTop: ture
date: 2023-07-12 14:13:38
updated: 2023-07-12 14:13:38
swiperImg:
bgImg:
img: https://s1.imagehub.cc/images/2023/07/12/2023-07-12-14.31.22.png
---

# 培训报告

**[学院组织]**

**[7.13-7.16]**

## 培训目的

本次技术培训的目的是提升参与人员在特定技术领域的知识和技能，以应对公司在该领域面临的挑战和需求。通过培训，参与人员将能够掌握相关技术，并将其应用于日常工作中，从而提高工作效率和质量。

[//]: # (## 培训安排)

[//]: # ()

[//]: # (### 日程安排)

[//]: # ()

[//]: # (| 日期  | 活动                                                                |)

[//]: # (|-----|-------------------------------------------------------------------|)

[//]: # (| 13号 | - 培训会议<br>- 技术演讲<br>- 小组讨论<br>- 午餐<br>- 实践项目<br>- 学习工作坊<br>- 自由活动 |)

[//]: # (| 14号 | - 项目演示<br>- 交流分享<br>- 专题研讨<br>- 午餐<br>- 实践练习<br>- 结束              |)

[//]: # (| 15号 | - 工作坊<br>- 案例分析<br>- 小组讨论<br>- 午餐<br>- 实践项目<br>- 知识分享<br>- 自由活动   |)

[//]: # (| 16号 | - 技术演示<br>- 研究报告<br>- 小组讨论<br>- 午餐<br>- 实践练习<br>- 总结讨论<br>- 结束    |)

[//]: # ()

[//]: # (## 培训师资)

[//]: # ()

[//]: # (明确培训的讲师或指导人员，并提供他们的背景和专业知识。确保讲师具备相关技术领域的专业能力和教学经验。)

[//]: # ()

[//]: # (## 培训参与人员)

[//]: # ()

[//]: # (|     |     |     |     |     |)

[//]: # (|-----|-----|-----|-----|-----|)

[//]: # (| 舒歆玥 | 袁思怡 | 李微微 | 范薷月 | 刘佳怡 |)

[//]: # (| 姜蕴桐 | 杨志成 | 李琦  | 李泉东 | 贾欣彤 |)

[//]: # (| 孙畅  | 宋广文 | 周志莹 | 曹立航 | 刘嘉诚 |)

[//]: # (| 安主恩 | 许定申 | 房敬博 | 绳韬  | 关诗凡 |)

[//]: # (| 贺一凡 | 鄢启迪 | 潘迪  | 岳馨雨 | 毛宇佳 |)

[//]: # (| 赵呈志 | 陈柏冰 | 乔诗媛 | 隋浩鑫 | 张芯芮 |)

[//]: # (| 姜智超 | 胡中来 |     |     |     |)

## 07-13 基础环境搭建

### 基础环境

**搭建步骤**

1. 修改主机名，便于识别节点；
   ```shell
   #  修改主机名
   hostnamectl set-hostname <hostname>
   # 刷新
   bash
   ```

2. 修改hosts文件，添加集群节点映射，按照给出的节点IP和对应的主机名进行设置；
   ```shell
   # 修改hosts文件内容
   vim /etc/hosts
   ```
   ```text
   # 【局域网ip】 【主机名】
   [ip] [hostname]
   ```

3. 要求各节点时区修改为中国时区（ 中国标准时间CST+8）
   ```shell
   # 修改为中国时区
   timedatectl set-timezone Asia/Shanghai
   ```

4. 安装ntp服务，要求主节点master为本地时钟源，从节点设置定时任务同步本地时间；
   ```shell
   # 修改master节点NTP配置
   vim /etc/ntp.conf
   ```
   设置master为本地时间服务器，屏蔽默认server，服务器层级设为10
   ```text
   # server <默认服务器>
   server 127.127.1.0
   fudge 127.127.1.0 stratum 10
   ```
   启动ntp服务
   ```shell
   systemctl start ntp
   systemctl restart ntpd.service
   ```
   添加定时任务--在早十-晚五时间段内每隔半个小时同步一次本地服务器时间（24小时制、使用用户root任务调度crontab，服务器地址使用主机名）
   ```shell
   sudo crontab -e
   ```
   ```text
   */30 10-17 * * * /usr/sbin/ntpdate -u <主机名>
   ```

5. 集群中数据传输需要节点之间免密访问，要求设置主节点之间到从节点的免密访问；
   主节点生成公钥文件id_rsa.pub(数字签名RSA，用户root，主机名master)
   ```shell
   ssh-keygen
   ```
   建⽴master⾃身使⽤root⽤户ssh访问localhost免密登录
   若没有authorized_keys则使用下面命令创建
   ```shell
   cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
   ```
   建⽴master使⽤root⽤户到slave的ssh免密登录访问
   ```shell
   ssh-copy-id root@<从节点IP或主机名>
   ```
   测试
   ```shell
   ssh-copy-id root@<从节点IP或主机名>
   ```

6. 配置Java环境
   解压下载的java JDK
   ```shell
   tar -zxvf [jdk]
   ```
   添加系统环境变量
   ```shell
   vim /etc/profile
   ```
   添加内容
   ```text
   export JAVA_HOME=<you_java_jdk_path>
   export PATH="$JAVA_HOME/bin:$PATH"
   ```
   配置生效
   ```shell
   source /etc/profile
   ```

7. 分发
   ```shell
   scp -r /etc/profile root@slave:/etc/
   scp -r /usr/java root@slave:/usr/
   ```

### Zookeeper

**概念**

ZooKeeper是一个开源的分布式协调服务，它为分布式应用程序提供了高度可靠的协调功能。它旨在解决分布式系统中的一些常见问题，如配置管理、命名服务、分布式锁、分布式协调等。

ZooKeeper的设计目标是提供一个简单而高效的分布式协调服务，它采用了基于观察者模式的数据模型。在ZooKeeper中，数据被组织为一个分层的命名空间（类似于文件系统的目录结构），称为ZooKeeper树（ZooKeeper
tree）或ZooKeeper命名空间（ZooKeeper namespace）。每个节点在树中都有一个唯一的路径标识，并可以存储一个小的数据块。

ZooKeeper提供了临时节点、顺序节点、观察者机制等特性，可以用于实现分布式锁、选主（Leader
Election）、集群管理、分布式队列等场景。它的核心是一个高可用的、基于主从架构的协调服务器集群，通过选举机制确保了服务的高可用性和可靠性。

在分布式系统中，ZooKeeper被广泛应用于各种场景，如大数据、分布式数据库、消息队列、分布式应用程序等。它为分布式应用程序提供了一个可靠的、高性能的基础设施，帮助开发人员简化了分布式系统的设计和实现。

Zookeeper是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。
预装的配置文件zoo_sample.cfg下面默认有五个属性，分别是：

1. tickTime
   心跳间隔，单位是毫秒，系统默认是2000毫秒，也就是间隔两秒心跳一次。
   tickTime的意义：客户端与服务器或者服务器与服务器之间维持心跳，也就是每个tickTime时间就会发送一次心跳。通过心跳不仅能够用来监听机器的工作状态，还可以通过心跳来控制Flower跟Leader的通信时间，默认情况下FL的会话时常是心跳间隔的两倍。

2. initLimit
   集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。

3. syncLimit
   集群中flower服务器（F）跟leader（L）服务器之间的请求和答应最多能容忍的心跳数。

4. clientPort
   客户端连接的接口，客户端连接zookeeper服务器的端口，zookeeper会监听这个端口，接收客户端的请求访问，端口默认是2181。

5. dataDir
   该属性对应的目录是用来存放myid信息跟一些版本，日志，跟服务器唯一的ID信息等。
   在集群Zookeeper服务在启动的时候，会回去读取zoo.cfg这个文件，从这个文件中找到这个属性然后获取它的值也就是dataDir
   的路径，它会从这个路径下面读取myid这个文件，从这个文件中获取要启动的当前服务器的地址。

集群信息的配置：
在配置文件中，配置集群信息是存在一定的格式：service.N =YYY： A：B
N：代表服务器编号（准确对应对应服务器中myid里面的值）
YYY：服务器地址
A：表示 Flower 跟 Leader的通信端口，简称服务端内部通信的端口（默认2888）
B：表示是选举端口（默认是3888）

参考配置文件

```text
tickTime=2000
initLimit=10
syncLimit=5
clientPort=2181
# 配置数据存储路径
????
# 配置日志文件路径
????
# 配置集群列表
server.1=????
server.2=????
server.3=????
```

**搭建步骤**

1. 将zookeeper安装包解压
   ```shell
   tar -zxvf [zookeeper]
   ```

2. 配置系统变量ZOOKEEPER_HOME，同时将Zookeeper安装路径中bin目录加入PATH系统变量
   ```shell
   vim /etc/profile
   ```
   ```text
   # zookeeper
   export ZOOKEEPER_HOME=/usr/zookeeper/zookeeper-3.4.14
   export PATH="$ZOOKEEPER_HOME/bin:$PATH"
   ```
   ```shell
   source /etc/profile
   ```
3. Zookeeper的默认配置文件为Zookeeper安装路径下conf/zoo_sample.cfg，将其修改为zoo.cfg
   ```shell
   cd zookeeper/zookeeper-3.4.14/conf/
   mv zoo_sample.cfg zoo.cfg
   ```

4. 设置数据存储路径(dataDir)为/usr/zookeeper/zookeeper-3.4.14/zkdata
   ```shell
   vim zoo.cfg
   ```
   ```text
   dataDir=/usr/zookeeper/zookeeper-3.4.14/zkdata
   ```

5. 设置日志文件路径(dataLogDir)为/usr/zookeeper/zookeeper-3.4.14/zkdatalog
   ```shell
   vim zoo.cfg
   ```
   ```text
   dataLogDir=/usr/zookeeper/zookeeper-3.4.14/zkdatalog
   ```

6. 设置集群列表（要求master为1号服务器，slave1为2号服务器，slave2为3号服务器）
   ```shell
   vim zoo.cfg
   ```
   ```text
   server.1=master:2888:3888
   server.2=slave1:2888:3888
   server.3=slave2:2888:3888
   ```

7. 创建所需数据存储文件夹、日志存储文件夹
   ```shell
   mkdir -p /usr/zookeeper/zookeeper-3.4.14/{zkdata,zkdatalog}
   ```

8. 数据存储路径下创建myid，写入对应的标识主机服务器序号
   ```shell
   cd /usr/zookeeper/zookeeper-3.4.14/zkdata
   vim myid
   ```
   server.1 中的 1 就是主机服务器的序号
   ```text
   1
   ```

9. 分发
   ```shell
   scp -r /usr/zookeeper/ root@slave:/usr
   scp /etc/profile root@slave:/etc
   ```

10. 启动服务，查看进程QuorumPeerMain是否存在
   ```shell
   zkServer.sh start
   ```

11. 查看各节点服务器角色是否正常(leader/follower)
   ```shell
   zkServer.sh status
   ```

### Hadoop

**概述**

Hadoop是一个开源的分布式计算框架，旨在处理大规模数据集的存储和处理。它提供了可靠性、可扩展性和容错性，适用于在集群中并行处理大量数据的场景。

Hadoop的核心组件包括：

1. Hadoop Distributed File System (HDFS)：这是一个分布式文件系统，用于存储大规模数据集。它将数据分散存储在集群中的多个节点上，并提供高容错性和高可靠性，以支持大规模数据的存储和处理。

2. MapReduce：这是一个分布式数据处理模型，用于在Hadoop集群上并行处理大规模数据。MapReduce将计算任务分解为可并行执行的“映射”（Map）和“归约”（Reduce）阶段，允许在分布式环境中处理海量数据。

除了核心组件之外，Hadoop生态系统还包括许多其他工具和组件，如：

- YARN (Yet Another Resource Negotiator)：作为Hadoop的资源管理器，负责集群资源的管理和任务调度。
- Hive：一个基于Hadoop的数据仓库和查询工具，提供类似于SQL的查询语言，用于分析和处理存储在Hadoop上的数据。
- Spark：一个快速、通用的大数据处理引擎，提供高级别的API，支持内存计算和更复杂的数据处理模式。
- HBase：一个分布式、可扩展的列式数据库，用于存储和访问结构化数据。
- Pig：一种高级数据流脚本语言，用于编写复杂的数据转换和分析任务。
- ZooKeeper：一个分布式协调服务，用于管理和协调分布式系统中的各种任务。

Hadoop被广泛应用于大数据领域，它可以处理海量数据并提供可靠的数据存储和分析能力。它的设计理念和架构使得它适用于构建可扩展的分布式系统，以满足日益增长的大数据需求。

Hadoop是由Java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架，其核心部件是HDFS与MapReduce。

1. HDFS是一个分布式文件系统：引入存放文件元数据信息的服务器Namenode和实际存放数据的服务器Datanode，对数据进行分布式储存和读取。
2. MapReduce是一个计算框架：MapReduce的核心思想是把计算任务分配给集群内的服务器里执行。通过对计算任务的拆分（Map计算/Reduce计算）再根据任务调度器（JobTracker）对任务进行分布式计算。

| 配置文件            | 	配置对象         | 	主要内容                                                           |
|-----------------|---------------|-----------------------------------------------------------------|
| hadoop-env.sh   | 	hadoop运行环境   | 	用来定义Hadoop运行环境相关的配置信息；                                         |
| core-site.xml   | 	集群全局参数       | 	定义系统级别的参数，包括HDFS URL、Hadoop临时目录等；                              |
| hdfs-site.xml	  | HDFS参数        | 	定义名称节点、数据节点的存放位置、文本副本的个数、文件读取权限等；                              |
| mapred-site.xml | 	MapReduce参数	 | 包括JobHistory Server 和应用程序参数两部分，如reduce任务的默认个数、任务所能够使用内存的默认上下限等； |
| yarn-site.xml   | 	集群资源管理系统参数	  | 配置ResourceManager ，nodeManager的通信端口，web监控端口等；                   |

Hadoop的配置类是由资源指定的，资源可以由一个String或Path来指定,资源以XML形式的数据表示，由一系列的键值对组成。资源可以用String或path命名（示例如下），

1. String:指示hadoop在classpath中查找该资源；
2. Path:指示hadoop在本地文件系统中查找该资源。

**配置示例**

```xml

<configuration>
    <property>
        <name>fs.default.name</name>
        <value>????</value>
    </property>
</configuration>
```

1. core-site.xml

   | 配置参数	            | 	说明                   |
   |------------------|-----------------------|
   | fs.default.name	 | 	用于指定NameNode的地址      |
   | hadoop.tmp.dir	  | 	Hadoop运行时产生文件的临时存储目录 |

2. hdfs-site.xml

   | 配置参数	                              | 	说明                               |
   |------------------------------------|-----------------------------------|
   | dfs.replication	                   | 	指定备份数                            |
   | dfs.namenode.name.dir              | 	NameNode在本地文件系统中持久存储命名空间和事务日志的路径 |
   | dfs.datanode.data.dir              | 	DataNode在本地文件系统中存放块的路径           |
   | dfs.permissions                    | 	集群权限系统校验                         |
   | dfs.datanode.use.datanode.hostname | 	datanode之间通过域名方式通信               |

   > 注意：外域机器通信需要用外网IP，未配置hostname访问会访问异常。可以在Java api客户端使用conf.set("fs.client.use.datanode.hostname","true");。

3. mapreduce-site.xml

   | 配置参数	                      | 	说明                                             |
   |----------------------------|-------------------------------------------------|
   | mapreduce.framework.name		 | 	指定执行MapReduce作业的运行时框架。属性值可以是local，classic或yarn |

4. yarn-site.xml

   | 配置参数	                                                 | 	说明                                                                                         |
   |-------------------------------------------------------|---------------------------------------------------------------------------------------------|
   | yarn.resourcemanager.admin.address	                   | 		用于指定RM管理界面的地址（主机:端口）                                                                      |
   | yarn.nodemanager.aux-services	                        | 	mapreduce 获取数据的方式，指定在进行mapreduce作业时，yarn使用mapreduce_shuffle混洗技术。这个混洗技术是hadoop的一个核心技术，非常重要。 |
   | yarn.nodemanager.auxservices.mapreduce.shuffle.class	 | 	用于指定混洗技术对应的字节码文件，值为org.apache.hadoop.mapred.ShuffleHandler                                 |

**配置步骤**

1. Hadoop安装包解压
   ```shell
   tar -zxvf [hadoop]
   ```
   
2. 配置环境变量HADOOP_HOME，将Hadoop安装路径中bin目录和sbin目录加入PATH系统变量
   ```shell
   vim /etc/profile
   ```
   ```text
   # hadoop
   export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7
   export PATH="$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH"
   ```
   ```shell
   source /etc/profile
   ```
   
3. 配置Hadoop运行环境JAVA_HOME
   ```shell
   cd /usr/hadoop/hadoop-2.7.7/etc/hadoop
   vim hadoop-env.sh
   ```
   ```text
   export JAVA_HOME=/usr/java/jdk1.8.0_221
   ```

4. 设置全局参数，指定HDFS上NameNode地址为master,端口默认为9000；指定临时存储目录为本地/root/hadoopData/tmp(要求为绝对路径，下同)
   ```shell
   vim core-site.xml
   ```
   ```xml
   <configuration>
       <property>
           <name>fs.default.name</name>
           <value>hdfs://master:9000</value>
       </property>
       <property>
           <name>hadoop.tmp.dir</name>
           <value>/root/hadoopData/tmp</value>
       </property>
   </configuration>
   ```

5. 设置HDFS参数，指定备份文本数量为2；设置HDFS参数，指定NN存放元数据信息路径为本地/root/hadoopData/name；指定DN存放元数据信息路径为本地/root/hadoopData/data(要求为绝对路径)；设置HDFS参数，关闭hadoop集群权限校验（安全配置），允许其他用户连接集群；指定datanode之间通过域名方式进行通信
   ```shell
   vim hdfs-site.xml
   ```
   ```xml
   <configuration>
       <property>
           <name>dfs.replication</name>
           <value>2</value>
       </property>
       <property>
           <name>dfs.namenode.name.dir</name>
           <value>/root/hadoopData/name</value>
       </property>
       <property>
           <name>dfs.datanode.data.dir</name>
           <value>/root/hadoopData/data</value>
       </property>
       <property>
           <name>dfs.permissions</name>
           <value>false</value>
       </property>
       <property>
           <name>dfs.datanode.use.datanode.hostname</name>
           <value>true</value>
       </property>
   </configuration>
   ```

6. 设置YARN运行环境$JAVA_HOME参数
   ```shell
   vim yarn-env.sh
   ```
   ```text
   export JAVA_HOME=/usr/java/jdk1.8.0_221
   ```

7. 设置YARN核心参数，指定ResourceManager进程所在主机为master，端口为18141;指定mapreduce 获取数据的方式为mapreduce_shuffle
   ```shell
   vim yarn-site.xml
   ```
   ```xml
   <configuration>
       <property>
           <name>yarn.resourcemanager.admin.address</name>
           <value>master:18141</value>
       </property>
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
       <property>
           <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
           <value>org.apache.hadoop.mapred.shuffleHandler</value>
       </property>
   </configuration>
   ```

8. 设置计算框架参数，指定MR运行在yarn上
   ```shell
   cp mapred-site.xml.template mapred-site.xml
   vim mapred-site.xml
   ```
   ```xml
   <configuration>
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ```

9. 设置节点文件，要求master为主节点； slave1、slave2为子节点
   这里填入从节点
   ```shell
   vim slaves
   ```
   ```text
   slave
   ```
   这里填入主节点
   ```shell
   vim master
   ```
   ```shell
   master
   ```

10. 分发
   ```shell
   scp -r /usr/hadoop/ root@slave:/usr/
   scp /etc/profile root@slave:/etc/
   ```

11. 对文件系统进行格式化
   ```shell
   hdfs namenode -format
   ```
   出现一下内容就是成功了
   ```text
   INFO common.Storage: Storage directory /root/hadoopData/name has been successfully formatted.
   ```

12. 启动Hadoop集群查看各节点服务
   ```shell
   start-all.sh 
   hadoop-daemon.sh start datanode
   jps
   ```

13. 查看集群运行状态是否正常
   ```shell
   hadoop dfsadmin -report
   ```

### Hive

**概念**

Hive是基于Hadoop的数据仓库基础架构，它提供了一种类似于SQL的查询语言（HiveQL）和用于处理大规模数据集的数据处理能力。Hive使得用户可以使用SQL语言进行数据查询、转换和分析，而无需编写复杂的MapReduce程序。

Hive的核心思想是将结构化查询语言（SQL）映射到Hadoop分布式文件系统（HDFS）上的大规模数据集。它将SQL查询转换为一系列MapReduce作业，从而能够在Hadoop集群上并行处理数据。

Hive的特性包括：

1. 数据模型：Hive提供了类似于关系型数据库的表结构，支持基于模式的数据存储和查询。它可以将数据映射到表和分区，并支持复杂的数据类型和数据模型。

2. 查询语言：Hive的查询语言HiveQL类似于SQL，允许用户使用SQL语句来查询和分析数据。HiveQL支持常见的查询操作，如SELECT、JOIN、GROUP
   BY、ORDER BY等。

3. 数据转换和ETL：Hive支持数据转换和ETL（抽取、转换、加载）操作，可以通过HiveQL进行数据清洗、转换和提取。

4. 扩展性和集成：Hive可以与其他Hadoop生态系统组件集成，如HDFS、HBase、Spark等。它还支持自定义函数（UDF）和扩展插件，允许用户编写自定义逻辑和扩展功能。

5. 元数据管理：Hive维护表和分区的元数据信息，包括表的结构、存储位置、分区信息等。这使得Hive能够提供更高级别的查询优化和查询计划生成。

Hive广泛应用于大数据领域，特别是数据仓库、数据分析和数据处理场景。它提供了一种简化的方式来使用SQL语言进行大数据查询和处理，使得更多的人可以轻松地利用Hadoop集群进行数据分析和数据挖掘。

1. 环境中已经安装mysql-community-server，注意mysql5.7默认安装后为root用户随机生成一个密码；
   直接查看密码：grep "temporary password" /var/log/mysqld.log
   登入数据库：mysql -uroot -p
   输入随机密码即可登录

2. 根据要求设置密码，注意对应的安全策略修改；
   设置密码强度为低级：set global validate_password_policy=????;
   设置密码长度：set global validate_password_length=????;
   修改本地密码：alter user 'root'@'localhost' identified by '????';

3. 根据要求满足任意主机节点root的远程访问权限(否则后续hive无法连接mysql)；
   ```sql
   GRANT ALL PRIVILEGES ON *.* TO '????'@'%' IDENTIFIED BY '????' WITH GRANT OPTION;
   ```
   
4. 注意刷新权限；
   ```sql
   flush privileges;
   ```

5. 参考命令
   1. 启动mysql服务：systemctl start mysqld.service
   2. 关闭mysql服务：systemctl stop mysqld.service
   3. 查看mysql服务：systemctl status mysqld.service

**配置步骤**

1. 环境中已经安装mysql-community-server，关闭mysql开机自启服务
   ```shell
   systemctl disable mysqld
   ```
   
2. 开启MySQL服务
   ```shell
   systemctl start mysqld
   ```

3. 判断mysqld.log日志下是否生成初临时密码
   ```shell
   grep "temporary password" /var/log/mysqld.log 
   ```
   观察初始密码并复制下来

4. 设置mysql数据库本地root用户密码为123456
   登录mysql，使用临时密码
   ```shell
   mysql -uroot -p
   ```
   sql中执行
   ```sql
   set global validate_password_policy=0;
   set global validate_password_length=4;
   alter user root@localhost identified by ‘123456’;
   ```
   退出
   ```sql
   quit
   ```

5. Hive安装包解压
   ```shell
   tar -zxvf [hive]
   ```
   
6. 配置环境变量HIVE_HOME,将Hive安装路径中的bin目录加入PATH系统变量
   ```shell
   vim /etc/profile
   ```
   ```text
   export HIVE_HOME=/usr/hive/apache-hive-2.3.4-bin
   export PATH=PATH : PATH:PATH:HIVE_HOME/bin
   ```
   ```shell
   source /etc/profile
   ```

7. 修改HIVE运行环境，配置Hadoop安装路径HADOOP_HOME；修改HIVE运行环境，配置Hive配置文件存放路径HIVE_CONF_DIR；修改HIVE运行环境，配置Hive运行资源库路径HIVE_AUX_JARS_PATH
   ```shell
   cd /usr/hive/apache-hive-2.3.4-bin/conf/
   cp hive-env.sh.template hive-env.sh
   vim hive-env.sh
   ```
   ```text
   export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7
   export HIVE_CONF_DIR=/usr/hive/apache-hive-2.3.4-bin/conf
   export HIVE_AUX_JARS_PATH=/usr/hive/apache-hive-2.3.4-bin/lib
   ```

8. 解决jline的版本冲突，将$HIVE_HOME/lib/jline-2.12.jar同步至$HADOOP_HOME/share/hadoop/yarn/lib/下
   ```shell
   cp /usr/package277/mysql-connector-java-5.1.47-bin.jar /usr/hive/apache-hive-2.3.4-bin/lib/
   ```
   
9. 分发
   ```shell
   scp -r /usr/hive root@slave:/usr
   scp /etc/profile root@slave:/etc
   ```

   {% note info, 接下来的操作请选择刚才进行分发的slave从节点进行操作 %}

10. 驱动JDBC拷贝至hive安装目录对应lib下（依赖包存放于/usr/package277/）
   ```shell
   cp /usr/package277/mysql-connector-java-5.1.47-bin.jar /usr/hive/apache-hive-2.3.4-bin/lib/
   ```

11. 配置元数据数据存储位置为/user/hive_remote/warehouse；配置连接JDBC的URL地址主机名及默认端口号3306，数据库为hive，如不存在自行创建，ssl连接方式为false；配置数据库连接用户；配置数据库连接密码
   ```shell
   vim /usr/hive/apache-hive-2.3.4-bin/conf/hive-site.xml
   ```
   ```xml
   <configuration>
      <!-- Hive产生的元数据存放位置-->
      <property>
         <name>hive.metastore.warehouse.dir</name>
         <value>/user/hive_remote/warehouse</value>
      </property>
      <!-- 数据库连接driver，即MySQL驱动-->
      <property>
         <name>javax.jdo.option.ConnectionDriverName</name>
         <value>root</value>
      </property>
      <!-- 数据库连接JDBC的URL地址-->
      <property>
         <name>javax.jdo.option.ConnectionURL</name>
         <value>jdbc:mysql://slave2:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
      </property>
      <!-- MySQL数据库用户名-->
      <property>
         <name>javax.jdo.option.ConnectionUserName</name>
         <value>com.mysql.jdbc.Driver</value>
      </property>
      <!-- MySQL数据库密码-->
      <property>
         <name>javax.jdo.option.ConnectionPassword</name>
         <value>123456</value>
      </property>
   </configuration>
   ```
   {% note info, 接下来的操作请选择master节点进行操作 %}

12. 配置元数据存储位置为/user/hive_remote/warehouse；关闭本地metastore模式；配置指向metastore服务的主机为slave1，端口为9083
   ```shell
   vim /usr/hive/apache-hive-2.3.4-bin/conf/hive-site.xml
   ```
   ````xml
   <configuration>
      <!-- Hive产生的元数据存放位置-->
      <property>
         <name>hive.metastore.warehouse.dir</name>
         <value>/user/hive_remote/warehouse</value>
      </property>
      <!--- 使用本地服务连接Hive,默认为true-->
      <property>
         <name>hive.metastore.local</name>
         <value>false</value>
      </property>
      <!-- 连接服务器-->
      <property>
         <name>hive.metastore.uris</name>
         <value>thrift://slave1:9083</value>
      </property>
   </configuration>
   ````

13. 服务器端初始化数据库，并启动metastore服务;
   ```shell
   schematool -dbType mysql -initSchema
   hive --service metastore
   ```

14. 客户端开启进入hive，创建hive数据库
   ```shell
   hive
   ```
   ```hive
   create database hive;
   ```

### Spark

Hive是基于Hadoop的数据仓库基础架构，它提供了一种类似于SQL的查询语言（HiveQL）和用于处理大规模数据集的数据处理能力。Hive使得用户可以使用SQL语言进行数据查询、转换和分析，而无需编写复杂的MapReduce程序。

Hive的核心思想是将结构化查询语言（SQL）映射到Hadoop分布式文件系统（HDFS）上的大规模数据集。它将SQL查询转换为一系列MapReduce作业，从而能够在Hadoop集群上并行处理数据。

Hive的特性包括：

1. 数据模型：Hive提供了类似于关系型数据库的表结构，支持基于模式的数据存储和查询。它可以将数据映射到表和分区，并支持复杂的数据类型和数据模型。

2. 查询语言：Hive的查询语言HiveQL类似于SQL，允许用户使用SQL语句来查询和分析数据。HiveQL支持常见的查询操作，如SELECT、JOIN、GROUP
   BY、ORDER BY等。

3. 数据转换和ETL：Hive支持数据转换和ETL（抽取、转换、加载）操作，可以通过HiveQL进行数据清洗、转换和提取。

4. 扩展性和集成：Hive可以与其他Hadoop生态系统组件集成，如HDFS、HBase、Spark等。它还支持自定义函数（UDF）和扩展插件，允许用户编写自定义逻辑和扩展功能。

5. 元数据管理：Hive维护表和分区的元数据信息，包括表的结构、存储位置、分区信息等。这使得Hive能够提供更高级别的查询优化和查询计划生成。

Hive广泛应用于大数据领域，特别是数据仓库、数据分析和数据处理场景。它提供了一种简化的方式来使用SQL语言进行大数据查询和处理，使得更多的人可以轻松地利用Hadoop集群进行数据分析和数据挖掘。

Spark是Hadoop的子项目。 环境中将Spark安装到基于Linux的系统中。

相关配置变量如下：

1. JAVA_HOME：Java安装目录 
2. HADOOP_HOME：Hadoop安装目录 
3. HADOOP_CONF_DIR：Hadoop集群的配置文件的目录 
4. SPARK_MASTER_IP：Spark集群的Master节点的ip地址 
5. SPARK_WORKER_MEMORY：每个worker节点能够最大分配给exectors的内存大小

**配置步骤**

1. 将Spark安装包解压
   ```shell
   tar -zvxf [spark]
   ```

2. 文件/etc/profile中配置环境变量SPARK_HOME，将Spark安装路径中的bin目录加入PATH系统变量
   ```shell
   vim /etc/profile
   ```
   ```text
   export SPARK_HOME=/usr/spark/spark-2.4.3-bin-hadoop2.7
   export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
   ```
   ```shell
   source /etc/profile
   ```

3. 修改配置文件spark-env.sh，设置主机节点为master，设置java安装路径，设置节点内存为8g，设置hadoop安装目录、hadoop集群的配置文件的目录，添加spark从节点
   ```shell
   cd /usr/spark/spark-2.4.3-bin-hadoop2.7/conf
   cp spark-defaults.sh.template spark-env.sh
   
   vim spark-env.sh
   ```
   ```text
   export JAVA_HOME=/usr/java/jdk1.8.0_221
   export SPARK_MASTER_IP=master
   export HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.7/etc/hadoop
   export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7
   export SPARK_WORKER_MEMORY=8g
   ```
   添加从节点
   ```shell
   cp slaves.template slaves
   vim slaves
   ```
   ```text
   slave
   ```
   
4. 分发
   ```shell
   scp -r /usr/spark root@slave:/usr
   scp /etc/profile root@slave:/etc
   ```

5. 开启集群，查看各节点进程(主节点进程为Master，子节点进程为Worker)
   ```shell
   cd /usr/spark/spark-2.4.3-bin-hadoop2.7/sbin
   ./start-all.sh
   ```

## 07-14 Hadoop MapReduce

## 07-15 Hive数据获取与分析

## 07-16 培训内容

## 培训评估

### 学习目标评估

在培训开始前，明确学习目标，并与参与人员共享。这有助于参与人员了解培训的预期成果，并衡量自身在培训过程中的学习进展。

### 反馈和评估

提供参与人员对培训的反馈渠道，如问卷调查、讨论会等。收集参与人员的意见和建议，以评估培训的有效性，并为未来的培训改进提供参考。

## 培训成果

说明参与人员在完成培训后预期达到的成果。可以包括技能提升、知识扩展、解决问题的能力提高等方面的预期效果。